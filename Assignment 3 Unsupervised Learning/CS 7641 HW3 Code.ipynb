{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 7641 HW1 Code - Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will provide analysis clustering and dimensionality reduction techniques for for two datasets.\n",
    "\n",
    "Datasets: Phishing Websites, Bank Marketing.\n",
    "\n",
    "Clustering Techniques: k-Means, Expectation Maximization.\n",
    "Dimensionality Reduction Techniques: PCA, ICA, RCA, RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Load and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data! Please save the datasets to your local machine and change the current directory to a file where you have the data stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "os.chdir(r\"C:\\Users\\westy\\OneDrive\\01 - CS 7641 - Machine Learning\\HW3 - Unsupervised\") #change this to your current working directory\n",
    "#os.chdir(r\"C:\\Users\\kwest18\\Desktop\\ML Code\") #change this to your current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Phishing Website Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 11055 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "## Download the Phishing Data from OpenML https://www.openml.org/d/4534\n",
    "\n",
    "df_phish = pd.read_csv('PhishingWebsitesData.csv').astype('category')\n",
    "print(\"Data has\",len(df_phish),\"rows and\", len(df_phish.columns),\"columns.\")\n",
    "if df_phish.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")\n",
    "#df_phish.head()\n",
    "#df_phish.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the phishing data is loaded, we need to do some preprocessing. Several of the columns are categorical with the levels {-1,0,1} and the rest are all binary with levels {-1,1}. For the 3-level columns we will use one-hot encoding to create additional features with level {0,1}. Finally, we will edit the binary features so that the new levels are all {0,1}. We will have more features now, but they will all be binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1hot = ['URL_Length','having_Sub_Domain','SSLfinal_State','URL_of_Anchor','Links_in_tags','SFH','web_traffic','Links_pointing_to_page']\n",
    "df_1hot = df_phish[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot)\n",
    "df_others = df_phish.drop(col_1hot,axis=1)\n",
    "df_phish = pd.concat([df_1hot,df_others],axis=1)\n",
    "df_phish = df_phish.replace(-1,0).astype('category')\n",
    "column_order = list(df_phish)\n",
    "column_order.insert(0, column_order.pop(column_order.index('Result')))\n",
    "df_phish = df_phish.loc[:, column_order]  #move the target variable 'Result' to the front\n",
    "#df_phish.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phish.to_csv(\"PhishingWebsitesData_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a file with no missing data in the format [y, X] where all features are binary {0,1}. The phishing data is ready to go! Now we move on to loading the Bank Marketing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bank Marketing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 45307 rows and 21 columns.\n"
     ]
    }
   ],
   "source": [
    "## Load the Bank Marketing Data from OpenML https://www.openml.org/d/1461\n",
    "\n",
    "df_bank = pd.read_csv('BankMarketingData.csv')\n",
    "print(\"Data has\",len(df_bank),\"rows and\", len(df_bank.columns),\"columns.\")\n",
    "if df_bank.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")\n",
    "#df_bank.head()\n",
    "#df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset needs some preprocessing love too. We will convert all categorical columns using one hot encoding. Additionally, we will standardize all of the numeric features and we will convert the target variable from {no,yes} to {0,1}. It should be noted that the feature 'pdays' is numeric but contains values that are '999' if the customer was not called before. It may be worth while to create a new feature that defines whether or not {0,1} a customer had been called before. In the current state the '999' values may be outliers. Finally we will standardize all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1hot = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "df_1hot = df_bank[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "df_others = df_bank.drop(col_1hot,axis=1)\n",
    "df_bank = pd.concat([df_others,df_1hot],axis=1)\n",
    "column_order = list(df_bank)\n",
    "column_order.insert(0, column_order.pop(column_order.index('y')))\n",
    "df_bank = df_bank.loc[:, column_order]\n",
    "df_bank['y'].replace(\"no\",0,inplace=True)\n",
    "df_bank['y'].replace(\"yes\",1,inplace=True)\n",
    "df_bank['y'] = df_bank['y'].astype('category')\n",
    "\n",
    "numericcols = ['age','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']\n",
    "df_num = df_bank[numericcols]\n",
    "df_stand =(df_num-df_num.min())/(df_num.max()-df_num.min())\n",
    "df_bank_categorical = df_bank.drop(numericcols,axis=1)\n",
    "df_bank = pd.concat([df_bank_categorical,df_stand],axis=1)\n",
    "#df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.to_csv(\"BankMarketingData_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded and processed both datasets. We are ready to start the ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Function Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the algorithms, let's define some helper functions that will be used across all of the models and both datasets. We will define a function to load the data (not really necessary in a Jupyter notebook, but good if this is exported as a .py for later use). We will also define a function that plots the learning curve (training and cross validation score as a function of training examples) of an estimator (classification model). Finally, we define functions to output final model scores using an untouched test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def import_data():\n",
    "\n",
    "    X1 = np.array(df_phish.values[:,1:-1],dtype='int64')\n",
    "    Y1 = np.array(df_phish.values[:,0],dtype='int64')\n",
    "    X2 = np.array(df_bank.values[:,1:-1],dtype='int64')\n",
    "    Y2 = np.array(df_bank.values[:,0],dtype='int64')\n",
    "    return X1, Y1, X2, Y2\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Untouched Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def cluster_predictions(Y,clusterLabels):\n",
    "    assert (Y.shape == clusterLabels.shape)\n",
    "    pred = np.empty_like(Y)\n",
    "    for label in set(clusterLabels):\n",
    "        mask = clusterLabels == label\n",
    "        sub = Y[mask]\n",
    "        target = Counter(sub).most_common(1)[0][0]\n",
    "        pred[mask] = target\n",
    "#    assert max(pred) == max(Y)\n",
    "#    assert min(pred) == min(Y)    \n",
    "    return pred\n",
    "\n",
    "def pairwiseDistCorr(X1,X2):\n",
    "    assert X1.shape[0] == X2.shape[0]\n",
    "    \n",
    "    d1 = pairwise_distances(X1)\n",
    "    d2 = pairwise_distances(X2)\n",
    "    return np.corrcoef(d1.ravel(),d2.ravel())[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will build a forward-feed neural network which computes weights via backpropagation (a multilayer perceptron). The main hyperparameter will be number of hidden nodes in a network defined by a single hidden layer, while others that could be searched over in grid search are activation function, and learning rate. This will be used later when we compare neural networks built from different combinations of features after clustering and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,150,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.05, random_state=100)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(accuracy_score(y_test, y_pred_test))\n",
    "            f1_train.append(accuracy_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train Accuracy')\n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test Accuracy')\n",
    "    plt.ylabel('Model Accuracy')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    param_grid = {'hidden_layer_sizes': h_units}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',learning_rate_init=0.05,random_state=100),\n",
    "                       param_grid=param_grid, cv=10)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(net.best_params_)\n",
    "    return net.best_params_['hidden_layer_sizes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will implement k-means clustering for both datasets. Our objectives are to:\n",
    "1. Determine the best number of clusters for each dataset by using the elbow inspection method on silhouette score.\n",
    "2. Describe the attributes which make up each cluster.\n",
    "3. Score each cluster with an accuracy since technically we do have labels available for these datasets (labels are not used when determining clusters).\n",
    "\n",
    "Since k-Means is susceptible to get stuck in local optima due to the random selection of initial cluster centers, I will report the average metrics over 5 models for each number of k clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score as sil_score, f1_score, homogeneity_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def run_kmeans(X,y,title):\n",
    "\n",
    "    kclusters = list(np.arange(2,50,2))\n",
    "    sil_scores = []; f1_scores = []; homo_scores = []; train_times = []\n",
    "\n",
    "    for k in kclusters:\n",
    "        start_time = timeit.default_timer()\n",
    "        km = KMeans(n_clusters=k, n_init=10,random_state=100,n_jobs=-1).fit(X)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        sil_scores.append(sil_score(X, km.labels_))\n",
    "        y_mode_vote = cluster_predictions(y,km.labels_)\n",
    "        f1_scores.append(f1_score(y, y_mode_vote))\n",
    "        homo_scores.append(homogeneity_score(y, km.labels_))\n",
    "        \n",
    "    # elbow curve for silhouette score\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kclusters, sil_scores)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Avg Silhouette Score')\n",
    "    plt.title('Elbow Plot for KMeans: '+ title)\n",
    "    plt.show()\n",
    "   \n",
    "    # plot homogeneity scores\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kclusters, homo_scores)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Homogeneity Score')\n",
    "    plt.title('Homogeneity Scores KMeans: '+ title)\n",
    "    plt.show()\n",
    "\n",
    "#     # plot f1 scores\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(kclusters, f1_scores)\n",
    "#     plt.grid(True)\n",
    "#     plt.xlabel('No. Clusters')\n",
    "#     plt.ylabel('F1 Score')\n",
    "#     plt.title('F1 Scores KMeans: '+ title)\n",
    "#     plt.show()\n",
    "\n",
    "#     # plot model training time\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(kclusters, train_times)\n",
    "#     plt.grid(True)\n",
    "#     plt.xlabel('No. Clusters')\n",
    "#     plt.ylabel('Training Time (s)')\n",
    "#     plt.title('KMeans Training Time: '+ title)\n",
    "#     plt.show()\n",
    "    \n",
    "def evaluate_kmeans(km, X, y):\n",
    "    start_time = timeit.default_timer()\n",
    "    km.fit(X, y)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    y_mode_vote = cluster_predictions(y,km.labels_)\n",
    "    auc = roc_auc_score(y, y_mode_vote)\n",
    "    f1 = f1_score(y, y_mode_vote)\n",
    "    accuracy = accuracy_score(y, y_mode_vote)\n",
    "    precision = precision_score(y, y_mode_vote)\n",
    "    recall = recall_score(y, y_mode_vote)\n",
    "    cm = confusion_matrix(y, y_mode_vote)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Mode Cluster Vote\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.2f}\".format(training_time))\n",
    "    print(\"No. Iterations to Converge: {}\".format(km.n_iter_))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "#run_kmeans(phishX,phishY,'Phishing Data')\n",
    "km = KMeans(n_clusters=9,n_init=10,random_state=100,n_jobs=-1)\n",
    "evaluate_kmeans(km,phishX,phishY)\n",
    "df = pd.DataFrame(km.cluster_centers_)\n",
    "df.to_csv(\"Phishing kMeans Cluster Centers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "#run_kmeans(bankX,bankY,'Banking Data')\n",
    "km = KMeans(n_clusters=25,n_init=10,random_state=100,n_jobs=-1)\n",
    "evaluate_kmeans(km,bankX,bankY)\n",
    "df = pd.DataFrame(km.cluster_centers_)\n",
    "df.to_csv(\"Banking kMeans Cluster Centers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will implement k-means clustering for both datasets. The same 3 objectives from k-means apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "from sklearn.metrics import silhouette_score as sil_score, f1_score, homogeneity_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def run_EM(X,y,title):\n",
    "\n",
    "    #kdist =  [2,3,4,5]\n",
    "    #kdist = list(range(2,51))\n",
    "    kdist = list(np.arange(2,100,5))\n",
    "    sil_scores = []; f1_scores = []; homo_scores = []; train_times = []; aic_scores = []; bic_scores = []\n",
    "    \n",
    "    for k in kdist:\n",
    "        start_time = timeit.default_timer()\n",
    "        em = EM(n_components=k,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(X)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        \n",
    "        labels = em.predict(X)\n",
    "        sil_scores.append(sil_score(X, labels))\n",
    "        y_mode_vote = cluster_predictions(y,labels)\n",
    "        f1_scores.append(f1_score(y, y_mode_vote))\n",
    "        homo_scores.append(homogeneity_score(y, labels))\n",
    "        aic_scores.append(em.aic(X))\n",
    "        bic_scores.append(em.bic(X))\n",
    "        \n",
    "    # elbow curve for silhouette score\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, sil_scores)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('Avg Silhouette Score')\n",
    "    plt.title('Elbow Plot for EM: '+ title)\n",
    "    plt.show()\n",
    "   \n",
    "    # plot homogeneity scores\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, homo_scores)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('Homogeneity Score')\n",
    "    plt.title('Homogeneity Scores EM: '+ title)\n",
    "    plt.show()\n",
    "\n",
    "    # plot f1 scores\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, f1_scores)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Scores EM: '+ title)\n",
    "    plt.show()\n",
    "\n",
    "    # plot model AIC and BIC\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, aic_scores, label='AIC')\n",
    "    ax.plot(kdist, bic_scores,label='BIC')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('Model Complexity Score')\n",
    "    plt.title('EM Model Complexity: '+ title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_EM(em, X, y):\n",
    "    start_time = timeit.default_timer()\n",
    "    em.fit(X, y)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    labels = em.predict(X)\n",
    "    y_mode_vote = cluster_predictions(y,labels)\n",
    "    auc = roc_auc_score(y, y_mode_vote)\n",
    "    f1 = f1_score(y, y_mode_vote)\n",
    "    accuracy = accuracy_score(y, y_mode_vote)\n",
    "    precision = precision_score(y, y_mode_vote)\n",
    "    recall = recall_score(y, y_mode_vote)\n",
    "    cm = confusion_matrix(y, y_mode_vote)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Mode Cluster Vote\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.2f}\".format(training_time))\n",
    "    print(\"No. Iterations to Converge: {}\".format(em.n_iter_))\n",
    "    print(\"Log-likelihood Lower Bound: {:.2f}\".format(em.lower_bound_))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "run_EM(phishX,phishY,'Phishing Data')\n",
    "em = EM(n_components=24,covariance_type='diag',n_init=1,warm_start=True,random_state=100)\n",
    "evaluate_EM(em,phishX,phishY)\n",
    "df = pd.DataFrame(em.means_)\n",
    "df.to_csv(\"Phishing EM Component Means.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.25)\n",
    "run_EM(X_train,y_train,'Banking Data')\n",
    "em = EM(n_components=41,covariance_type='diag',n_init=1,warm_start=True,random_state=100)\n",
    "evaluate_EM(em,bankX,bankY)\n",
    "df = pd.DataFrame(em.means_)\n",
    "df.to_csv(\"Banking EM Component Means.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will implement 4 different dimensionality reduction techniques on both the phishing and the banking dataset. Then, k-means and EM clustering will be performed for each (dataset * dim_reduction) combination to see how the clustering compares with using the full datasets. The 4 dimensionality reduction techniques are:\n",
    "- Principal Components Analysis (PCA). Optimal number of PC chosen by inspecting % variance explained and the eigenvalues.\n",
    "- Independent Components Analysis (ICA). Optimal number of IC chosen by inspecting kurtosis.\n",
    "- Random Components Analysis (RCA) (otherwise known as Randomized Projections). Optimal number of RC chosen by inspecting reconstruction error.\n",
    "- Random Forest Classifier (RFC). Optimal number of components chosen by feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA as ICA\n",
    "from sklearn.random_projection import GaussianRandomProjection as GRP, SparseRandomProjection as RCA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "def run_PCA(X,y,title):\n",
    "    \n",
    "    pca = PCA(random_state=5).fit(X) #for all components\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(list(range(len(pca.explained_variance_ratio_))), cum_var, 'b-')\n",
    "    ax1.set_xlabel('Principal Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Cumulative Explained Variance Ratio', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(list(range(len(pca.singular_values_))), pca.singular_values_, 'm-')\n",
    "    ax2.set_ylabel('Eigenvalues', color='m')\n",
    "    ax2.tick_params('y', colors='m')\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.title(\"PCA Explained Variance and Eigenvalues: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_ICA(X,y,title):\n",
    "    \n",
    "    dims = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    dims.append(X.shape[1])\n",
    "    ica = ICA(random_state=5)\n",
    "    kurt = []\n",
    "\n",
    "    for dim in dims:\n",
    "        ica.set_params(n_components=dim)\n",
    "        tmp = ica.fit_transform(X)\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp = tmp.kurt(axis=0)\n",
    "        kurt.append(tmp.abs().mean())\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"ICA Kurtosis: \"+ title)\n",
    "    plt.xlabel(\"Independent Components\")\n",
    "    plt.ylabel(\"Avg Kurtosis Across IC\")\n",
    "    plt.plot(dims, kurt, 'b-')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def run_RCA(X,y,title):\n",
    "    \n",
    "    dims = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    dims.append(X.shape[1])\n",
    "    tmp = defaultdict(dict)\n",
    "\n",
    "    for i,dim in product(range(5),dims):\n",
    "        rp = RCA(random_state=i, n_components=dim)\n",
    "        tmp[dim][i] = pairwiseDistCorr(rp.fit_transform(X), X)\n",
    "    tmp = pd.DataFrame(tmp).T\n",
    "    mean_recon = tmp.mean(axis=1).tolist()\n",
    "    std_recon = tmp.std(axis=1).tolist()\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(dims,mean_recon, 'b-')\n",
    "    ax1.set_xlabel('Random Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Mean Reconstruction Correlation', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dims,std_recon, 'm-')\n",
    "    ax2.set_ylabel('STD Reconstruction Correlation', color='m')\n",
    "    ax2.tick_params('y', colors='m')\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.title(\"Random Components for 5 Restarts: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_RFC(X,y,df_original):\n",
    "    rfc = RFC(n_estimators=500,min_samples_leaf=round(len(X)*.01),random_state=5,n_jobs=-1)\n",
    "    imp = rfc.fit(X,y).feature_importances_ \n",
    "    imp = pd.DataFrame(imp,columns=['Feature Importance'],index=df_original.columns[2::])\n",
    "    imp.sort_values(by=['Feature Importance'],inplace=True,ascending=False)\n",
    "    imp['Cum Sum'] = imp['Feature Importance'].cumsum()\n",
    "    imp = imp[imp['Cum Sum']<=0.95]\n",
    "    top_cols = imp.index.tolist()\n",
    "    return imp, top_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "run_PCA(phishX,phishY,\"Phishing Data\")\n",
    "run_ICA(phishX,phishY,\"Phishing Data\")\n",
    "run_RCA(phishX,phishY,\"Phishing Data\")\n",
    "imp_phish, topcols_phish = run_RFC(phishX,phishY,df_phish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.2)\n",
    "run_PCA(X_train,bankY,\"Banking Data\")\n",
    "run_ICA(X_train,bankY,\"Banking Data\")\n",
    "run_RCA(X_train,bankY,\"Banking Data\")\n",
    "imp_bank, topcols_bank = run_RFC(X_train,y_train,df_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating Clustering Experiment (k-means and EM) for phishing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "imp_phish, topcols_phish = run_RFC(phishX,phishY,df_phish)\n",
    "pca_phish = PCA(n_components=22,random_state=5).fit_transform(phishX)\n",
    "ica_phish = ICA(n_components=38,random_state=5).fit_transform(phishX)\n",
    "rca_phish = ICA(n_components=29,random_state=5).fit_transform(phishX)\n",
    "rfc_phish = df_phish[topcols_phish]\n",
    "rfc_phish = np.array(rfc_phish.values,dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kmeans(pca_phish,phishY,'PCA Phishing Data')\n",
    "run_kmeans(ica_phish,phishY,'ICA Phishing Data')\n",
    "run_kmeans(rca_phish,phishY,'RCA Phishing Data')\n",
    "run_kmeans(rfc_phish,phishY,'RFC Phishing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_kmeans(KMeans(n_clusters=14,n_init=10,random_state=100,n_jobs=-1),pca_phish,phishY)\n",
    "evaluate_kmeans(KMeans(n_clusters=12,n_init=10,random_state=100,n_jobs=-1),ica_phish,phishY)\n",
    "evaluate_kmeans(KMeans(n_clusters=10,n_init=10,random_state=100,n_jobs=-1),rca_phish,phishY)\n",
    "evaluate_kmeans(KMeans(n_clusters=14,n_init=10,random_state=100,n_jobs=-1),rfc_phish,phishY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_EM(pca_phish,phishY,'PCA Phishing Data')\n",
    "run_EM(ica_phish,phishY,'ICA Phishing Data')\n",
    "run_EM(rca_phish,phishY,'RCA Phishing Data')\n",
    "run_EM(rfc_phish,phishY,'RFC Phishing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_EM(EM(n_components=18,covariance_type='diag',n_init=1,warm_start=True,random_state=100),pca_phish,phishY)\n",
    "evaluate_EM(EM(n_components=28,covariance_type='diag',n_init=1,warm_start=True,random_state=100),ica_phish,phishY)\n",
    "evaluate_EM(EM(n_components=25,covariance_type='diag',n_init=1,warm_start=True,random_state=100),rca_phish,phishY)\n",
    "evaluate_EM(EM(n_components=22,covariance_type='diag',n_init=1,warm_start=True,random_state=100),rfc_phish,phishY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating Clustering Experiment (k-means and EM) for banking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_bank = PCA(n_components=20,random_state=5).fit_transform(X_train)\n",
    "ica_bank = ICA(n_components=47,random_state=5).fit_transform(X_train)\n",
    "rca_bank = ICA(n_components=39,random_state=5).fit_transform(X_train)\n",
    "rfc_bank = df_bank[topcols_bank]\n",
    "rfc_bank = np.array(rfc_bank.values,dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kmeans(pca_bank,y_train,'PCA Banking Data')\n",
    "run_kmeans(ica_bank,y_train,'ICA Banking Data')\n",
    "run_kmeans(rca_bank,y_train,'RCA Banking Data')\n",
    "run_kmeans(rfc_bank,y_train,'RFC Banking Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_kmeans(KMeans(n_clusters=20,n_init=10,random_state=100,n_jobs=-1),pca_bank,y_train)\n",
    "evaluate_kmeans(KMeans(n_clusters=47,n_init=10,random_state=100,n_jobs=-1),ica_bank,y_train)\n",
    "evaluate_kmeans(KMeans(n_clusters=10,n_init=10,random_state=100,n_jobs=-1),rca_bank,y_train)\n",
    "evaluate_kmeans(KMeans(n_clusters=14,n_init=10,random_state=100,n_jobs=-1),rfc_bank,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_EM(pca_bank,bankY,'PCA Banking Data')\n",
    "run_EM(ica_bank,bankY,'ICA Banking Data')\n",
    "run_EM(rca_bank,bankY,'RCA Banking Data')\n",
    "run_EM(rfc_bank,bankY,'RFC Banking Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_EM(EM(n_components=18,covariance_type='diag',n_init=1,warm_start=True,random_state=100),pca_bank,bankY)\n",
    "evaluate_EM(EM(n_components=28,covariance_type='diag',n_init=1,warm_start=True,random_state=100),ica_bank,bankY)\n",
    "evaluate_EM(EM(n_components=25,covariance_type='diag',n_init=1,warm_start=True,random_state=100),rca_bank,bankY)\n",
    "evaluate_EM(EM(n_components=22,covariance_type='diag',n_init=1,warm_start=True,random_state=100),rfc_bank,bankY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Neural Network on Projected Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will train a neural network on the 4 projected datasets for only the phishing data. We will examine the learning curves on the training data as well as the final network performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original, full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20)\n",
    "full_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_full, NN_train_score_full, NN_fit_time_full, NN_pred_time_full = plot_learning_curve(full_est, X_train, y_train,title=\"Neural Net Phishing: Full\")\n",
    "final_classifier_evaluation(full_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(pca_phish),np.array(phishY), test_size=0.20)\n",
    "pca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_pca, NN_train_score_pca, NN_fit_time_pca, NN_pred_time_pca = plot_learning_curve(pca_est, X_train, y_train,title=\"Neural Net Phishing: PCA\")\n",
    "final_classifier_evaluation(pca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(ica_phish),np.array(phishY), test_size=0.20)\n",
    "ica_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_ica, NN_train_score_ica, NN_fit_time_ica, NN_pred_time_ica = plot_learning_curve(ica_est, X_train, y_train,title=\"Neural Net Phishing: ICA\")\n",
    "final_classifier_evaluation(ica_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(rca_phish),np.array(phishY), test_size=0.20)\n",
    "rca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rca, NN_train_score_rca, NN_fit_time_rca, NN_pred_time_rca = plot_learning_curve(rca_est, X_train, y_train,title=\"Neural Net Phishing: RCA\")\n",
    "final_classifier_evaluation(rca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(rfc_phish),np.array(phishY), test_size=0.20)\n",
    "rfc_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rfc, NN_train_score_rfc, NN_fit_time_rfc, NN_pred_time_rfc = plot_learning_curve(rfc_est, X_train, y_train,title=\"Neural Net Phishing: RFC\")\n",
    "final_classifier_evaluation(rfc_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Comparison Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and call a function that will plot training times and learning rates for the 4 different NN models so that we can compare across the classifiers for the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fit_time(n,full_fit,pca_fit,ica_fit,rca_fit,rfc_fit,title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Training Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Training Time (s)\")\n",
    "    plt.plot(n, full_fit, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_fit, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_fit, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_fit, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_fit, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def compare_pred_time(n,full_pred, pca_pred, ica_pred, rca_pred, rfc_pred, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Prediction Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Prediction Time (s)\")\n",
    "    plt.plot(n, full_pred, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_pred, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_pred, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_pred, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_pred, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_learn_time(n,full_learn, pca_learn, ica_learn, rca_learn, rfc_learn, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Learning Rates: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.plot(n, full_learn, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_learn, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_learn, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_learn, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_learn, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_full, NN_fit_time_full, NN_fit_time_pca, NN_fit_time_ica, \n",
    "                 NN_fit_time_rca, NN_fit_time_rfc, 'Phishing Dataset')              \n",
    "compare_pred_time(train_samp_full, NN_pred_time_full, NN_pred_time_pca, NN_pred_time_ica, \n",
    "                 NN_pred_time_rca, NN_pred_time_rfc, 'Phishing Dataset')   \n",
    "compare_learn_time(train_samp_full, NN_train_score_full, NN_train_score_pca, NN_train_score_ica, \n",
    "                 NN_train_score_rca, NN_train_score_rfc, 'Phishing Dataset')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Neural Network on Projected Data with Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will train a neural network on the 4 projected datasets for only the phishing data. The difference in this section is that we now add cluster labels from both k-means and EM (after 1-hot encoding) to the reduced datasets. We will examine the learning curves on the training data as well as the final network performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addclusters(X,km_lables,em_lables):\n",
    "    \n",
    "    df = pd.DataFrame(X)\n",
    "    df['KM Cluster'] = km_labels\n",
    "    df['EM Cluster'] = em_labels\n",
    "    col_1hot = ['KM Cluster', 'EM Cluster']\n",
    "    df_1hot = df[col_1hot]\n",
    "    df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "    df_others = df.drop(col_1hot,axis=1)\n",
    "    df = pd.concat([df_others,df_1hot],axis=1)\n",
    "    new_X = np.array(df.values,dtype='int64')   \n",
    "    \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=9,n_init=10,random_state=100,n_jobs=-1).fit(phishX)\n",
    "km_labels = km.labels_\n",
    "em = EM(n_components=24,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(phishX)\n",
    "em_labels = em.predict(phishX)\n",
    "\n",
    "clust_full = addclusters(phishX,km_labels,em_labels)\n",
    "clust_pca = addclusters(pca_phish,km_labels,em_labels)\n",
    "clust_ica = addclusters(ica_phish,km_labels,em_labels)\n",
    "clust_rca = addclusters(rca_phish,km_labels,em_labels)\n",
    "clust_rfc = addclusters(rfc_phish,km_labels,em_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original, full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_full),np.array(phishY), test_size=0.20)\n",
    "full_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_full, NN_train_score_full, NN_fit_time_full, NN_pred_time_full = plot_learning_curve(full_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: Full\")\n",
    "final_classifier_evaluation(full_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_pca),np.array(phishY), test_size=0.20)\n",
    "pca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_pca, NN_train_score_pca, NN_fit_time_pca, NN_pred_time_pca = plot_learning_curve(pca_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: PCA\")\n",
    "final_classifier_evaluation(pca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_ica),np.array(phishY), test_size=0.20)\n",
    "ica_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_ica, NN_train_score_ica, NN_fit_time_ica, NN_pred_time_ica = plot_learning_curve(ica_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: ICA\")\n",
    "final_classifier_evaluation(ica_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_rca),np.array(phishY), test_size=0.20)\n",
    "rca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rca, NN_train_score_rca, NN_fit_time_rca, NN_pred_time_rca = plot_learning_curve(rca_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: RCA\")\n",
    "final_classifier_evaluation(rca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_rfc),np.array(phishY), test_size=0.20)\n",
    "rfc_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rfc, NN_train_score_rfc, NN_fit_time_rfc, NN_pred_time_rfc = plot_learning_curve(rfc_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: RFC\")\n",
    "final_classifier_evaluation(rfc_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate new datasets with cluster labels added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_full, NN_fit_time_full, NN_fit_time_pca, NN_fit_time_ica, \n",
    "                 NN_fit_time_rca, NN_fit_time_rfc, 'Phishing Dataset')              \n",
    "compare_pred_time(train_samp_full, NN_pred_time_full, NN_pred_time_pca, NN_pred_time_ica, \n",
    "                 NN_pred_time_rca, NN_pred_time_rfc, 'Phishing Dataset')   \n",
    "compare_learn_time(train_samp_full, NN_train_score_full, NN_train_score_pca, NN_train_score_ica, \n",
    "                 NN_train_score_rca, NN_train_score_rfc, 'Phishing Dataset')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
